[TOC]

### 1. 神经网络的知识点

* 神经网络中的激活函数不是真的要去“激活”什么东西，其实际作用是给神经网络加入一些**非线性因素**，增强模型的表达能力，使得神经网络可以解决较为复杂的问题。
* 一个**n维**空间中的**超平面**是一个**n-1维**的子空间。比如一个平面（2维）的超平面表现为一条直线（1维），一个三维空间的超平面表现为一个面（2维）。
* 有进行函数处理（有激活函数）的神经元，称为**功能神经元**。如输出层神经元、隐含层神经元。

***

### 2. 范数的理解

给定向量x = (x<sub>1</sub>，x<sub>2</sub>，..., x<sub>n</sub>)，则有：

* **L<sub>1</sub>范数**：向量各个元素绝对值之和；
* **L<sub>2</sub>范数**：向量各个元素的平方求和然后求平方根；
* **L<sub>p</sub>范数**：向量各个元素绝对值的p次方求和然后求1/p次方；
* **L<sub>∞</sub>范数**：向量各个元素求绝对值，绝对值最大的那个元素的绝对值。

***

### 3. 机器学习中Batch Size、Iteration和Epoch

* **Batch Size**：批尺寸。
* **Iteration**：迭代，可以理解为w和b的一次更新，就是一次Iteration。
* **Epoch**：样本中的所有样本数据被计算一次就叫做一个Epoch。

***

### 4. 机器学习中参数更新的方法

有三种：

* **Batch Gradient Descent（批梯度下降）**。遍历全部数据集计算一次损失函数，进行一次参数更新，这样得到的方向能够更加准确的指向极值的方向，但是计算开销大，速度慢。
* **Stochastic Gradient Descent（随机梯度下降）**。对每一个样本计算一次损失函数，进行一次参数更新，优点是速度快，缺点是方向波动大，忽东忽西，不能准确地指向极值的方向，有时甚至两次更新相互抵消。
* **Mini-batch Gradient Decent（小批梯度下降）**。前面两种方法的折中，把样本数据分为若干批，分批来计算损失函数和更新参数，这样方向比较稳定，计算开销也相对较小。Batch Size就是每一批的样本数量。

***

### 5. 推导公式中的arg min, arg max的含义

* **arg**是变元（即自变量argument）的英文缩写。
* **arg min** 就是使后面这个式子达到最小值时的变量的取值。
* **arg max** 就是使后面这个式子达到最大值时的变量的取值。

> 例如 函数F(x,y):
>
> arg min F(x,y)就是指当F(x,y)取得最小值时，变量x,y的取值；
>
> arg max F(x,y)就是指当F(x,y)取得最大值时，变量x,y的取值。

***

